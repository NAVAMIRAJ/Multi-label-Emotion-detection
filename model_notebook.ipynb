{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxL-SKliN-La"
   },
   "source": [
    "# Multi_label Emotion Classification\n",
    "Dataste used: **track-a.csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHEmzZuLCKRM"
   },
   "source": [
    "**Imports necessary libraries for the project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oLT0EfwaiXJ"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup, RobertaTokenizer, RobertaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report, hamming_loss, roc_curve, auc, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3tzKLWgCgx3"
   },
   "source": [
    "Reads the data from a CSV file `track-a.csv` into pandas DataFrame and all the rows with no emotions are removed.\n",
    "CSV file has 5 emotions `'anger', 'fear', 'joy', 'sadness', 'surprise'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOHHp-biaj2q"
   },
   "outputs": [],
   "source": [
    "# Read the data from csv file\n",
    "df = pd.read_csv('track-a.csv')\n",
    "\n",
    "#class of emotions\n",
    "label_columns = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
    "\n",
    "# drop rows without any emotion label\n",
    "df = df[df[label_columns].sum(axis=1) > 0].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faI3z0tgDHAc"
   },
   "source": [
    "Loaded data is splitted into training(75%), validation(15%), and testing(10%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YxBR4JCnb9L"
   },
   "outputs": [],
   "source": [
    "#splitting data for train(75%), validation(15%) and test(10%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.25, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.40, random_state=42)\n",
    "\n",
    "train_texts, train_labels = train_df['text'].tolist(), train_df[label_columns].values\n",
    "val_texts,   val_labels   = val_df['text'].tolist(),   val_df[label_columns].values\n",
    "test_texts,  test_labels  = test_df['text'].tolist(),  test_df[label_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_C4w3U5CnfXL"
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "tokenizer_name = 'roberta-base' #name of the pre-trained tokenizer(Roberta)\n",
    "max_len= 128                    # Maximum sequence length for tokenization\n",
    "batch_size = 16                 # batch size used for training and validaton\n",
    "\n",
    "lr_encoder= 1e-5      # Learning rate for the encoder part of the model(Roberta)\n",
    "lr_head = 5e-5        # Learning rate for the classification head\n",
    "\n",
    "epochs = 10                  # Total number of training epochs\n",
    "unfreeze_last4_epoch= 1      # Epoch at which to unfreeze the last 4 layers of the encoder\n",
    "unfreeze_all_epoch = 3       # Epoch at which to unfreeze the entire encoder\n",
    "\n",
    "#early stopping parameter\n",
    "best_val_f1 = 0.0             #tracks the best val F1 score\n",
    "best_val_loss = float('inf')  #tracks the best val loss score\n",
    "epochs_no_improve = 0         # Counts the number of epochs with no improvement in val loss\n",
    "patience = 2                  # Number of epochs to wait for improvement before early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euiW5sIMDdlg"
   },
   "source": [
    "Downloads and loads Roberta pre-trained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281,
     "referenced_widgets": [
      "b4689731413d4bf281c251ac6e4f303d",
      "1c3b58959d864268b1b34acb67e2a5e4",
      "e79a5c34737241c98c3e215a5f32aa84",
      "b457cbb6b5f94260aa7a3713a1e4883f",
      "c46042a8c028423fb3f6e56cbab15d1e",
      "d422467acf8b4eedbfc1109ff52585aa",
      "c26f9128be9f4cc5b65d5509199c7b45",
      "a89642d0ff854f2a9d1b41f84ad99be9",
      "b15243ed7a1349479ec49e2773a1dd3a",
      "13a43ef6951e4cf4b91c1d27a02bff7a",
      "efab9f706d5c494cb3d96013142f513a",
      "47d49a6fb21f4975a4a77ad471633414",
      "127dc08c600c404a87d4b9287026b0d0",
      "a8938834c234487d8971ce63ed81257b",
      "1b2855286e934972b4276c01d0f812db",
      "1933f5509701444fb8340fd2f9f1faa7",
      "e58c10eed95d4a1c8416a571967b45f0",
      "e9e871e9055a41dfabe89bea18de1791",
      "7728e29ff10f4718866e80928886733e",
      "ab66823dc4454f00b0d87512201ab5c4",
      "8d24309398ce4c66839e131a074ee05a",
      "ef2c5978c6cd49279e3eeaf86edd632e",
      "9b1da9edc45747349ecae75542de0353",
      "91eb8daab80845a8850971e0483b73ef",
      "9bb7af1750eb4de584dc25291270a9a8",
      "d9d966af8b364f58a554c9f691bef77d",
      "309d1ab5cb5a47c3ba944a3dd709251e",
      "c124c626e9f444d386caa207b4cccb10",
      "01d9599533954976adb10b5e3a22512f",
      "022343c8120148d2bf62efff03dd4fe7",
      "7faf73d10e95480a9dfc442f76e74b49",
      "dba03d08bf7d4aa08dd2c75e52fd1367",
      "73a9b165e6e84c069a6fd818cc02f21a",
      "af921637e9124eb7951229c4d84f4e89",
      "bf2d5d7c9be14c119e294400b9e2051f",
      "addc1d73337e4a67a35611a45cb25dc0",
      "8d6277adb3c943e4a8567ea8b367ae0d",
      "729726a2c4e9415281fb6f75f3f1cee2",
      "a8c50722bd874000b6925010753c2b3e",
      "2115a4f7334b49d483ecd6c3093bcd94",
      "aec1322e619a46bb8bf616434e2e90f4",
      "2385557993e749b8935873b7903b51b3",
      "697039c8437348bb8c2db716f5ac1e86",
      "072f99223bfa41d484754ab043297ace",
      "c2b6b06e6445420db5775aeec7f3d32a",
      "7d24bb37de7d4e538ab4fc03797ba6b2",
      "b3aee34f14194b439db849166037f594",
      "7b671c57683841fc99d7b3864c0f0692",
      "436a4d6d41db487097d089e436a44c04",
      "22208917f845420eb3d68f2eeb376fa8",
      "c5629a9dac594d808ff4d63467e8ce7e",
      "0c38e89402e34cf2bc3a522c133d4edc",
      "8c04c13576f44bff82b68a2f39efa417",
      "e29f7a4b4f5b44d6bb11d4ef43aa2aba",
      "8f2e45aadf404d93af994b0be4c11f48"
     ]
    },
    "id": "qc-nAUnmnspr",
    "outputId": "91d5863c-422b-41ee-81e5-35f2615db762"
   },
   "outputs": [],
   "source": [
    "#define roberta tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1nOt0pSDrmq"
   },
   "source": [
    "`EmotionDataset` class handles the tokenization of text data and prepares it in a format suitable for training the PyTorch model. Also creates instances of this dataset for the training, validation, and test sets and creates DataLoaders for efficient batching during training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNNscG8talkX"
   },
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels.astype('float32')\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        item['text'] = self.texts[idx]\n",
    "        return item\n",
    "\n",
    "\n",
    "\n",
    "train_dataset= EmotionDataset(train_texts,train_labels, tokenizer, max_len)\n",
    "val_dataset = EmotionDataset(val_texts,val_labels, tokenizer, max_len)\n",
    "test_dataset = EmotionDataset(test_texts, test_labels, tokenizer, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ER_ZKCr6EC_4"
   },
   "source": [
    "`EmotionClassifier` class uses a pre-trained Roberta model as the encoder and adds a dropout layer and a linear classifier for classifing into 5 labels on top for multi-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0g7z6s6CaqUR"
   },
   "outputs": [],
   "source": [
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=len(label_columns)):\n",
    "        super().__init__()\n",
    "        self.bert = RobertaModel.from_pretrained(tokenizer_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self.dropout(outputs.pooler_output)\n",
    "        return self.classifier(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4Td84I4EcBC"
   },
   "source": [
    "Functions for freezing and unfreezing layers of the Roberta model. These functions are used to implement fine-tuning, where different parts of the model are trained with different learning rates and are layers are kept frozen during early stages of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBA__ZEQar9J"
   },
   "outputs": [],
   "source": [
    "def freeze_all(model):\n",
    "    for p in model.bert.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "def unfreeze_last_k_layers(model, k):\n",
    "    freeze_all(model)\n",
    "    if k == 0:\n",
    "        return\n",
    "    for layer in model.bert.encoder.layer[-k:]:\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "def unfreeze_everything(model):\n",
    "    for p in model.bert.parameters():\n",
    "        p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNBBmVLtExki"
   },
   "source": [
    "Defines\n",
    "1. loss function: `BCEWithLogitsLoss`\n",
    "2. optimizer: `AdamW`\n",
    "3. learning rate scheduler: `get_linear_schedule_with_warmup`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "b542f72aff7c465c8a6832c7af1f19c4",
      "8ec027de01714e18be1bc57fa0371e55",
      "2b12a93c1db548f789cb6a10ed78f62b",
      "238e8beea8714036aa924a975eecf748",
      "715aafff493946b2895445c4e699220a",
      "bc60412be9e8464b9c7a44af794e934f",
      "e14efcfb00ea4f0b9227513814dbd95d",
      "129be1583990413b9049499702872bac",
      "c1bbf255784746aa80b3b62fa990f68b",
      "48cdf06104244bad84f39b74d15a5dba",
      "70dca5f0905649dba217e4de1d21efb6"
     ]
    },
    "id": "rOpuvEWZaszZ",
    "outputId": "a3ab14d2-d86f-4d53-870a-9bed186f12d0"
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = EmotionClassifier().to(device)\n",
    "\n",
    "# weighted class\n",
    "# label_counts = train_df[label_columns].sum()\n",
    "# neg_counts   = len(train_df) - label_counts\n",
    "# pos_weight   = torch.tensor(neg_counts / label_counts, dtype=torch.float32).to(device)\n",
    "# criterion    = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "criterion    = BCEWithLogitsLoss()\n",
    "\n",
    "# Parameter groups for discriminative LR\n",
    "optimizer = AdamW([\n",
    "    {\"params\": model.bert.parameters(), \"lr\": lr_encoder},\n",
    "    {\"params\": model.classifier.parameters(), \"lr\": lr_head}\n",
    "], weight_decay=0.01)\n",
    "\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler   = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# freezing all encoders\n",
    "freeze_all(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYSq58asFjXi"
   },
   "source": [
    "Training function `run_epoch` performs training or evaluation at each epoch. It iterates through the data loader, performs forward and backward passes (in training mode), calculates the loss, and computes evaluation metrics (macro F1 score and accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODWyYkVHayxj"
   },
   "outputs": [],
   "source": [
    "train_loss_hist, val_loss_hist = [], []\n",
    "train_acc_hist, val_acc_hist = [], []\n",
    "train_f1_hist,  val_f1_hist  = [], []\n",
    "\n",
    "\n",
    "def run_epoch(model, loader, train_mode=True):\n",
    "    if train_mode:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    total_loss, all_preds, all_labels = 0.0, [], []\n",
    "\n",
    "    for batch in tqdm(loader, leave=False):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attn_mask = batch['attention_mask'].to(device)\n",
    "        labels    = batch['labels'].to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(train_mode):\n",
    "            outputs = model(input_ids, attn_mask)\n",
    "            loss    = criterion(outputs, labels)\n",
    "            if train_mode:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_preds.append(torch.sigmoid(outputs).detach().cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    preds = torch.vstack(all_preds)\n",
    "    labels = torch.vstack(all_labels)\n",
    "    preds_bin = (preds > 0.5).int()\n",
    "    macro_f1 = f1_score(labels, preds_bin, average='macro')\n",
    "    exact_acc = np.mean((preds_bin.numpy() == labels.numpy()).all(axis=1))\n",
    "\n",
    "    return total_loss / len(loader), macro_f1, exact_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVorp8ZmF8o9"
   },
   "source": [
    "**Training loop** \\\n",
    "It iterates through the 10 epochs, implements the unfreezing schedule for the model layers, calls the `run_epoch` function for training and validation, tracks loss and train, val metrics, and implements early stopping based on validation loss. It also saves the model with the best validation F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451,
     "referenced_widgets": [
      "a449dbc523e74e9b887967dbc154df1c",
      "b120210236dc490b9208c58051a31625",
      "9c228c1cffb742818c7009dd5a283137",
      "48969412d9e9491c9e3c71719eefac45",
      "fc8e60da2f9e4f59a8ca53ff183e6a58",
      "4c3446d126d34ccca7cbd5fdee017e7c",
      "7bdb5bd5b67b4158965ed557ba4b940f",
      "5d0098c0a2604543b097e041d9def6bd",
      "bb8e82d1a80d4407b11869dc8fbf71b7",
      "49c63574013e461385e0e0c3191c4807",
      "0222c89a88a74ed689b5f37a6ad051b7",
      "e3687cef28704a598b662d41edcdae3b",
      "701d8a38d0f24f52a15fa1e597b698ff",
      "d4a02f9d79284717ad01c3a443b59e1d",
      "39e446ef68b6479abf72ae60f2aa2a90",
      "750b8bf6ecc244bc931ced922fd4583f",
      "156f406aba944195809fea5bda7b6d8f",
      "f4bc30c80aaa41a99d20b070352d1810",
      "bc9cb4bdac7a4176bbf8acdc5b5d5233",
      "b19963799bbc4009bb270af37c3ebc79",
      "1e30293018fd4583aa80e38f3f6b55a1",
      "513585b3b7b1457f8f3ef91018c546ef",
      "caada19622be42e1a3b82a9b8169647f",
      "e2e23313e2ec41b0af948a9344c68b0e",
      "639bf34589904d41ad5a9ada731f13a8",
      "793d11872d244681bc0911588584ba2b",
      "ff21fd8b525e4202b2246ef47fc33184",
      "853d4295e0024059a2eb449f4df0a135",
      "ed2f30059fbc487f80438978b6a42556",
      "9b5898bf31a34369856cd397f5c83ce8",
      "717dd00dc90149f0b9229f7afff9208a",
      "73079ae03f2540ec9bcab96c1d9bacd3",
      "cdc235bce7d64d9daa49f11a020c4def",
      "226e5ef8ac3f41a289f9552ee287ef72",
      "40f3ede85d5e4a09afa73b557f9d2db8",
      "2443df1cbe904b0aafeec72fc714c485",
      "314ea3eb46e2446e8e55b86be784c623",
      "f0402276e62045199f2c62cafc012e12",
      "470e8dc65bdb4b61bf1363ff4cbec9ed",
      "d4b5da556827465c9dd4f7347a37205b",
      "fd64bb46b35e4a4995b5c121216ad767",
      "e463dd5ed3144db081b9fbe57b44642e",
      "fdfbcba103dc4953a823855e310898d9",
      "48e5b5a305da4f3aa7268d5c140d07d7",
      "6bea1b3c219a48818dea711842029c84",
      "26503838c3e54e7e804f321c2b45ba41",
      "4fd4704677c540779f762950df10ee00",
      "952783f0fefd4d2a9fb93a089dc69095",
      "744b332249984aa49f492341ae4c371a",
      "d615bce09a804751af04e05db1be8b0f",
      "f3d15997a62849f99b46239135b70dfa",
      "a61aea4bd74147a2953ab40b5bbf5c46",
      "ab0ad1396f8b4e58921a756cd669855c",
      "0f21ee578bf94e0ea091ade07354d0e3",
      "95d49082ca2d436db506a508eea759ee",
      "c7f9199006ac46c7b77a9614e768789a",
      "44032a329f6649db8a0b3293c3cf8c2e",
      "21a6464f619940c09efa75e5ab70b812",
      "cedaf0b3222f45e883b2997cbec455f7",
      "c5cce5300e24481baa1fd3995827a4ff",
      "bc17a6d391db486b8a42d2b9cfd86418",
      "56fcd5e4d4934601ad95bb56445f37d2",
      "c08dbeca5e164db1905d67db5d6bda86",
      "e8f04098ee214307bc7264e7eeefbba8",
      "d5babdea98974fabb53d986b02a6c69e",
      "f78bc59143834678aa81a56847dfe967",
      "79977f7c53ae4530b5d904774dcd928c",
      "7619f08fd176438591606add3a7bbcf7",
      "0092cf70c9aa4a5a8bf4319e4efd7ca6",
      "08fc0178c23144a7a67d63fe86e7b681",
      "4be3a04508354e7d8f3d393e9299f966",
      "6874fd6c1174414e80cb5eab72a1d770",
      "6b2e85ad8936465e881f1bebe1b50960",
      "063a5240adcf44c59b3f3f2e6841f425",
      "d1b6e91d0a0242d581374597e0dc7591",
      "b2767485b42c414ba9335bfad28da28f",
      "d75457beff31432583b5927791414c77",
      "ec4430a0887b457db2f0a93aa916e830",
      "cc469a75213143adaad2ed3dadb04db2",
      "d8de97d2843a40538418119518d407f0",
      "97a4e170c551414a9ea65d5bd97fa455",
      "62db4e91e7814759922944d82bbf4e64",
      "1c2b8bc9eb0d467599d414e3d0615b20",
      "1bf4e25903864492a42548ba7b1615bb",
      "7ba194ed9ae343c58ef148fdc61f78da",
      "35b479f000604e1cb30c386195eb47d2",
      "d13beeaa93424407b825121f54fa72cf",
      "b2aec90b451948fdbe28f37d6aed7196",
      "29ae964f130c48c3ab4e902aa184c7ad",
      "1fa65f9ee279479a8de7756bdbbcdefa",
      "df3d98f24bea4ed6a239d3fc3b4d4941",
      "2755be96497043edb241f7ff6d5c3749",
      "0190be5b0f4049d5adff476e6f2e2157",
      "53c4c5394a154dc59d9415af3892fc00",
      "13f037ccd71f40d4bd2cd5dbd8756693",
      "4673cd94906b436d9a5cf03a45b6d5b7",
      "0fd1c470807b42c0aca61e99a2e71701",
      "15e8c694021440fc903ffb5f8a5a23d8",
      "54fcd68dbfec4b8289b47feb27de3c96",
      "d609d304c3d14b47a4b0957e4ac1485c",
      "7ea869dc9f40487fba762e735122dc04",
      "636911144dcc4459a340829e426fcf7c",
      "951a68e951f243c4b66267f40a6a4e8c",
      "809dac4e35d843f5b889560f603b1ea2",
      "7ceb10574d5b4e1d8a73461dd7e83fbf",
      "d4ec68a5d0274e66b289dfe5385bc9b5",
      "4b0e0e0035a4418cb3f11b32aba27546",
      "743a86b8eea04efe852ec92384dad8e8",
      "4f185ef55d75434b92a27edc32d7e42c",
      "e4e12239e165424385beeb4e30cc002a",
      "c871d11fc02341dabd409f6e3a13cbd7",
      "5db87d17b67f4b19bcd6e3d5b2e8a6b9",
      "33084932b1d2462290a122e8617b7d98",
      "de322a411e3c4074a956350a42475bff",
      "528f0232d765441fb570e75cdd0469b2",
      "d4921c2cf0094d08822c0e2a12786596",
      "5eb37f0d87af4a78b910d7a5827a65a6",
      "34109d4de0ae4d438017d0a7747096c6",
      "86b73d27d34640afb747126105b86f2e",
      "d0983d072d824332818d8f16dc7c2cf0",
      "2b3fdc895f294acc9125b7bf851969b3",
      "f8379c0570cf4a5ab9509d5502ae34ff",
      "85478ab8b6c2412ca634e12e341e7e69",
      "9be8bf95d46c442a98999989022b0291",
      "885b8d53d3cd476d89461c8ad81fb93c",
      "8f23750ef17c41d6ac656229c2022db0",
      "a4792428aee342cb9aa54deb29ee0d0b",
      "f25873bf024c41868c3ac2e9dd6fd928",
      "683799ca006648ca81830ceac62787b2",
      "92c77aa8d397428abb2ff9d0a3f5e33b",
      "2c34617e42bb40499ae0bb6d344bebce",
      "11a7b362c579490da296b9576fef9b35",
      "370af9393bf2496f8c99338662a45922",
      "cd60f922b8c2480296285d7a5b032026",
      "3ae25dd361264b2680ed62c1cbdb52e3",
      "3ddb5960bffb42d1b23ab5580cbdf494",
      "9889856dd9564fd0b7dc06d68575df81",
      "609a3571229b4c83b2ec29a4bb7e51b1",
      "e1d8e096a4364e1ebbf4e24d99da870c",
      "b470fd967f9d4428b672a288d51025f6",
      "3bc09f4c817a4a61bd96dac6f42282ae",
      "b3894a62343a48a4bd8c1f9876444fd3",
      "23430a5bf8d146a4b6a961a4b2535094",
      "853f92bb57764c008ccdc6faac26caed",
      "b943da5e7dff47f498150afc384f9e49",
      "c720f7a551da4c2eb406e9884b3b094a",
      "758a9e6b9aed40d692316933144946c1",
      "33462bb07ef04b36b136f9e1d6e193f7",
      "03455d0ad0924df88a2d78c2a8f77f3d",
      "e05c5e2268e24dc0b3a413aadd20bca3",
      "9c8b3a0f095e405fbdcf43e39cd86cae",
      "e78eba20f9d44d03937984223b1d4ad5",
      "0ecc7da6fa3e4094ab63ede735540b87",
      "6b785950b2e549b399452bae25814d8c"
     ]
    },
    "id": "o48EpxMTc7Kc",
    "outputId": "751f51f2-b9c1-435c-cc06-a1145b2ba050"
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Unfreezing schedule\n",
    "    if epoch == unfreeze_last4_epoch:\n",
    "        unfreeze_last_k_layers(model, 4)\n",
    "    if epoch == unfreeze_all_epoch:\n",
    "        unfreeze_everything(model)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    tr_loss, tr_f1, tr_acc,  = run_epoch(model, train_loader, train_mode=True)\n",
    "    vl_loss, vl_f1, vl_acc = run_epoch(model, val_loader, train_mode=False)\n",
    "\n",
    "    train_loss_hist.append(tr_loss)\n",
    "    val_loss_hist.append(vl_loss)\n",
    "    train_acc_hist.append(tr_acc)\n",
    "    val_acc_hist.append(vl_acc)\n",
    "    train_f1_hist.append(tr_f1)\n",
    "    val_f1_hist.append(vl_f1)\n",
    "\n",
    "    print(f\"Train loss={tr_loss:.4f} | Train acc: {tr_acc:.4f} | Train F1={tr_f1:.4f} | Val F1={vl_f1:.4f} | Val acc={vl_acc:.4f} | Val loss={vl_loss:.4f}\\n\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    train_stop = False\n",
    "    if vl_loss < best_val_loss:\n",
    "        best_val_loss = vl_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement in validation loss for {epochs_no_improve} epochs.\")\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            train_stop = True\n",
    "\n",
    "    # save best model by val_F1\n",
    "    if vl_f1 > best_val_f1:\n",
    "        best_val_f1 = vl_f1\n",
    "        torch.save(model.state_dict(), 'bert_emotion_best.pt')\n",
    "        tokenizer.save_pretrained(\"bert_emotion_best/\")\n",
    "\n",
    "    if train_stop:\n",
    "        break\n",
    "\n",
    "print(f\"Training finished with best validation macroâ€‘F1: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fPS345TNj4J"
   },
   "source": [
    "After training, got a best validation accuracy of 47% and F1 score of 72%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIJC1ueHGPpi"
   },
   "source": [
    "Function defined to calculate the `hamming score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACpt_NH2rAxd"
   },
   "outputs": [],
   "source": [
    "def hamming_score(y_true, y_pred):\n",
    "\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        true_set = set(np.where(y_true[i] == 1)[0])\n",
    "        pred_set = set(np.where(y_pred[i] == 1)[0])\n",
    "\n",
    "        if len(true_set) == 0 and len(pred_set) == 0:\n",
    "            acc_list.append(1) # if both empty->perfect match\n",
    "        else:\n",
    "            acc_list.append(len(true_set & pred_set) / len(true_set | pred_set))\n",
    "\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94W3a8h8a3ZW",
    "outputId": "61091d3e-4a00-4070-fd1a-2e51b0ef39fc"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('bert_emotion_best.pt'))\n",
    "model.eval()\n",
    "\n",
    "def evaluate(loader):\n",
    "    true, pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attn_mask = batch['attention_mask'].to(device)\n",
    "            labels    = batch['labels']\n",
    "            outputs   = model(input_ids, attn_mask)\n",
    "            probs     = torch.sigmoid(outputs).cpu()\n",
    "            pred.append((probs > 0.5).int())\n",
    "            true.append(labels)\n",
    "    return torch.vstack(true), torch.vstack(pred)\n",
    "\n",
    "true_test, pred_test = evaluate(test_loader)\n",
    "\n",
    "print(\"Test metrics:\")\n",
    "print(\"Accuracy:\", accuracy_score(true_test, pred_test))\n",
    "print(\"Macro F1:\", f1_score(true_test, pred_test, average='macro'))\n",
    "print(\"Precision:\", precision_score(true_test, pred_test, average='macro'))\n",
    "print(\"Recall:\", recall_score(true_test, pred_test, average='macro'))\n",
    "print('Hamming score:', hamming_score(true_test, pred_test))\n",
    "print(\"Hamming loss:\", hamming_loss(true_test, pred_test))\n",
    "print(\"Classification Report:\",classification_report(true_test, pred_test, target_names=label_columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7uxAo77J7md"
   },
   "source": [
    "Plot train-Val accuracy, Loss, ROC curve and F1 graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "usWSaGq63I5g",
    "outputId": "e322e260-d610-49de-82e0-7797ad5cac35"
   },
   "outputs": [],
   "source": [
    "#Plot train-val loss\n",
    "plt.plot(train_loss_hist, label='Train Loss')\n",
    "plt.plot(val_loss_hist, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "Src_7kV_JpyJ",
    "outputId": "fbe13d08-6dcf-44a5-9475-8f505fba66ff"
   },
   "outputs": [],
   "source": [
    "#plot train-val accuracy\n",
    "plt.plot(train_acc_hist, label='Train Accuracy')\n",
    "plt.plot(val_acc_hist, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "lJYs27tsJtG0",
    "outputId": "27219d30-7c61-4dc4-e9b3-14fbe32511a5"
   },
   "outputs": [],
   "source": [
    "\n",
    "#plot F1 score for train-val\n",
    "plt.plot(train_f1_hist, label='Train F1')\n",
    "plt.plot(val_f1_hist, label='Validation F1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 718
    },
    "id": "plrGwiXULrSk",
    "outputId": "b1168205-29c7-4a88-f538-bde3ce064b37"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate ROC curve and AUC for each emotions\n",
    "roc_data = {}\n",
    "for i, label in enumerate(label_columns):\n",
    "    fpr, tpr, thresholds = roc_curve(true_test[:, i], pred_test[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_data[label] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label, data in roc_data.items():\n",
    "    plt.plot(data['fpr'], data['tpr'], label=f'{label} (AUC = {data[\"auc\"]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Each Emotion')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrices = multilabel_confusion_matrix(true_test, pred_test)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 14))  \n",
    "axes = axes.flatten()\n",
    "\n",
    "#compute confusion matrix for each label\n",
    "for i, label in enumerate(label_columns):\n",
    "    cm = conf_matrices[i]\n",
    "    ax = axes[i]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)\n",
    "    ax.set_title(f'{label}')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_xticklabels(['No', 'Yes'])\n",
    "    ax.set_yticklabels(['No', 'Yes'], rotation=0)\n",
    "\n",
    "# Hide the extra subplot if label count < total subplots\n",
    "if len(label_columns) < len(axes):\n",
    "    for j in range(len(label_columns), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Confusion Matrices for Each Emotion Label', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate ROC curve and AUC for each emotions\n",
    "roc_data = {}\n",
    "for i, label in enumerate(label_cols):\n",
    "    fpr, tpr, thresholds = roc_curve(true_test[:, i], pred_test[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_data[label] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label, data in roc_data.items():\n",
    "    plt.plot(data['fpr'], data['tpr'], label=f'{label} (AUC = {data[\"auc\"]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Each Emotion')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
